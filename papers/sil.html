<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>From Snapshot to Stream — Dongjun Kim</title>
    <link rel="stylesheet" href="../styles.css" />
  </head>
  <body>
    <main class="container paper-main">
      <a href="../index.html#publications" class="project-link"
        >← Back to Publications</a
      >
      <h1 class="section-title page-title">
        From Snapshot to Stream: A Self-Improving Leaderboard for Robust and
        Evolving Natural Language Processing (NLP) Evaluation
      </h1>
      <p class="publication-authors">
        Chanjun Park, Hyeonseok Moon, <strong>Dongjun Kim</strong>, Seolhwa Lee,
        Jaehyung Seo, Sugyeong Eo, Heuiseok Lim
      </p>
      <p class="publication-venue">
        arXiv Preprint, 2025 —
        <a
          href="https://arxiv.org/abs/2303.10888"
          target="_blank"
          rel="noopener noreferrer"
          >arXiv</a
        >
      </p>

      <figure class="pub-figure-placeholder">
        <img
          alt="SIL paper figure"
          src="./images/sil.png"
          loading="lazy"
          decoding="async"
        />
      </figure>

      <div style="margin-top: 1rem">
        <h3 style="margin-bottom: 0.5rem; color: #000000">Abstract</h3>
        <div
          class="publication-abstract"
          style="display: block; margin-top: 0.5rem"
        >
          <p style="color: #334155">
            As natural language processing (NLP) systems are increasingly
            deployed in real-world environments, concerns have emerged regarding
            the relevance and reliability of traditional leaderboard-based
            evaluations. Existing leaderboards typically rely on static test
            sets and single-point evaluations, offering limited insight into a
            model's robustness, adaptability, and long-term utility. In this
            paper, we challenge the prevailing paradigm of benchmark-centric
            evaluation by identifying three structural limitations: (i) reliance
            on a fixed test distribution, (ii) misalignment between evaluation
            settings and real-world data dynamics, and (iii) overfitting
            incentives induced by leaderboard competition. To address these
            issues, we propose the Self-Improving Leaderboard (SIL), a
            conceptual framework that redefines model evaluation as a temporally
            evolving process. Rather than treating evaluation as a one-time
            snapshot, SIL maintains a dynamic test set that changes over time
            and supports longitudinal assessment of model performance. Our goal
            is not merely to introduce a new system, but to initiate a broader
            discussion about how the research community should rethink the
            design and purpose of leaderboard infrastructure. We argue that
            incorporating temporal variation, noise robustness, and
            distributional shift into evaluation is essential for aligning
            research progress with real-world demands.
          </p>
        </div>
      </div>
    </main>
  </body>
</html>
