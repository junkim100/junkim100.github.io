<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Biases in LLMs (Korean Context) — Dongjun Kim</title>
    <link rel="stylesheet" href="../styles.css" />
  </head>
  <body>
    <main class="container paper-main">
      <a href="../index.html#publications" class="project-link"
        >← Back to Publications</a
      >
      <h1 class="section-title page-title">
        Exploring Inherent Biases in LLMs within Korean Social Context: A
        Comparative Analysis of ChatGPT and GPT-4
      </h1>
      <p class="publication-authors">
        Seungyoon Lee, <strong>Dongjun Kim</strong>, Dahyun Jung, Chanjun Park,
        Heuiseok Lim
      </p>
      <p class="publication-venue">
        NAACL 2024 SRW —
        <a
          href="https://aclanthology.org/2024.naacl-srw.11/"
          target="_blank"
          rel="noopener noreferrer"
          >ACL Anthology</a
        >
      </p>

      <figure class="pub-figure-placeholder">
        <img
          alt="Korean Biases paper figure"
          src="./images/korean-biases.png"
          loading="lazy"
          decoding="async"
        />
      </figure>

      <div style="margin-top: 1rem">
        <h3 style="margin-bottom: 0.5rem; color: #000000">Abstract</h3>
        <div
          class="publication-abstract"
          style="display: block; margin-top: 0.5rem"
        >
          <p>
            Large Language Models (LLMs) have significantly impacted various
            fields requiring advanced linguistic understanding, yet concerns
            regarding their inherent biases and ethical considerations have also
            increased. Notably, LLMs have been critiqued for perpetuating
            stereotypes against diverse groups based on race, sexual
            orientation, and other attributes. However, most research analyzing
            these biases has predominantly focused on communities where English
            is the primary language, neglecting to consider the cultural and
            linguistic nuances of other societies. In this paper, we aim to
            explore the inherent biases and toxicity of LLMs, specifically
            within the social context of Korea. We devise a set of prompts that
            reflect major societal issues in Korea and assign varied personas to
            both ChatGPT and GPT-4 to assess the toxicity of the generated
            sentences. Our findings indicate that certain personas or prompt
            combinations consistently yield harmful content, highlighting the
            potential risks associated with specific persona-issue alignments
            within the Korean cultural framework. Furthermore, we discover that
            GPT-4 can produce more than twice the level of toxic content than
            ChatGPT under certain conditions.
          </p>
        </div>
      </div>
    </main>
  </body>
</html>
