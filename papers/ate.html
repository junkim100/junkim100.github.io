<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Enhancing Automatic Term Extraction — Dongjun Kim</title>
    <link rel="stylesheet" href="../styles.css" />
  </head>
  <body>
    <main class="container paper-main">
      <a href="../index.html#publications" class="project-link"
        >← Back to Publications</a
      >
      <h1 class="section-title page-title">
        Enhancing Automatic Term Extraction with Large Language Models via
        Syntactic Retrieval
      </h1>
      <p class="publication-authors">
        Yong Chan Chun, Minhyuk Kim, <strong>Dongjun Kim</strong>, Chanjun Park,
        Heuiseok Lim
      </p>
      <p class="publication-venue">
        Findings of ACL 2025 —
        <a
          href="https://arxiv.org/abs/2506.21222"
          target="_blank"
          rel="noopener noreferrer"
          >arXiv</a
        >
      </p>

      <figure class="pub-figure-placeholder">
        <img
          alt="ATE paper figure"
          src="./images/ate.png"
          loading="lazy"
          decoding="async"
        />
      </figure>

      <div style="margin-top: 1rem">
        <h3 style="margin-bottom: 0.5rem; color: #000000">Abstract</h3>
        <div
          class="publication-abstract"
          style="display: block; margin-top: 0.5rem"
        >
          <p style="color: #334155">
            Automatic Term Extraction (ATE) identifies domain-specific
            expressions that are crucial for downstream tasks such as machine
            translation and information retrieval. Although large language
            models (LLMs) have significantly advanced various NLP tasks, their
            potential for ATE has scarcely been examined. We propose a
            retrieval-based prompting strategy that, in the few-shot setting,
            selects demonstrations according to syntactic rather than semantic
            similarity. This syntactic retrieval method is domain-agnostic and
            provides more reliable guidance for capturing term boundaries. We
            evaluate the approach in both in-domain and cross-domain settings,
            analyzing how lexical overlap between the query sentence and its
            retrieved examples affects performance. Experiments on three
            specialized ATE benchmarks show that syntactic retrieval improves
            F1-score. These findings highlight the importance of syntactic cues
            when adapting LLMs to terminology-extraction tasks.
          </p>
        </div>
      </div>
    </main>
  </body>
</html>
