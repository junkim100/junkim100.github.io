<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>KITE — Dongjun Kim</title>
    <link rel="stylesheet" href="../styles.css" />
  </head>
  <body>
    <main class="container paper-main">
      <a href="../index.html#publications" class="project-link"
        >← Back to Publications</a
      >
      <h1 class="section-title page-title">
        KITE: A Benchmark for Evaluating Korean Instruction-Following Abilities
        in Large Language Models
      </h1>
      <p class="publication-authors">
        <strong>Dongjun Kim</strong>, Chanhee Park, Chanjun Park, Heuiseok Lim
      </p>
      <p class="publication-venue">
        arXiv Preprint, 2025 — IEEE Access 2026 Under Review —
        <a
          href="https://arxiv.org/abs/2510.15558"
          target="_blank"
          rel="noopener noreferrer"
          >arXiv</a
        >
        —
        <a
          href="https://huggingface.co/datasets/junkim100/KITE"
          target="_blank"
          rel="noopener noreferrer"
          >Dataset</a
        >
        —
        <a
          href="https://github.com/junkim100/KITE"
          target="_blank"
          rel="noopener noreferrer"
          >Code</a
        >
      </p>

      <figure class="pub-figure-placeholder">
        <img
          alt="KITE paper figure"
          src="./images/kite.png"
          loading="lazy"
          decoding="async"
        />
      </figure>

      <div style="margin-top: 1rem">
        <h3 style="margin-bottom: 0.5rem; color: #000000">Abstract</h3>
        <div
          class="publication-abstract"
          style="display: block; margin-top: 0.5rem"
        >
          <p style="color: #334155">
            The instruction-following capabilities of large language models
            (LLMs) are pivotal for numerous applications, from conversational
            agents to complex reasoning systems. However, current evaluations
            predominantly focus on English models, neglecting the linguistic and
            cultural nuances of other languages. Specifically, Korean, with its
            distinct syntax, rich morphological features, honorific system, and
            dual numbering systems, lacks a dedicated benchmark for assessing
            open-ended instruction-following capabilities. To address this gap,
            we introduce the Korean Instruction-following Task Evaluation
            (KITE), a comprehensive benchmark designed to evaluate both general
            and Korean-specific instructions. Unlike existing Korean benchmarks
            that focus mainly on factual knowledge or multiple-choice testing,
            KITE directly targets diverse, open-ended instruction-following
            tasks. Our evaluation pipeline combines automated metrics with human
            assessments, revealing performance disparities across models and
            providing deeper insights into their strengths and weaknesses. By
            publicly releasing the KITE
            <a
              href="https://huggingface.co/datasets/junkim100/KITE"
              target="_blank"
              rel="noopener noreferrer"
              >dataset</a
            >
            and
            <a
              href="https://github.com/junkim100/KITE"
              target="_blank"
              rel="noopener noreferrer"
              >code</a
            >, we aim to foster further research on culturally and
            linguistically inclusive LLM development and inspire similar
            endeavors for other underrepresented languages.
          </p>
        </div>
      </div>
    </main>
  </body>
</html>
